# -*- coding: utf-8 -*-
"""cloud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wl4XZCUUTIRVbGIrLJwPGnw-OwKO8llW

# Data Preprocessing & EDA

## Attaching Resources

### Importing Necessary Libraries
"""

from google.colab import files
uploaded=files.upload()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as pe
import statsmodels.api as sm
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import warnings

warnings.filterwarnings('ignore')
plt.rcParams['figure.figsize'] = (12,2)
plt.rcParams['figure.dpi'] = 250
sns.set_style(style='darkgrid')
plt.tight_layout()
# %matplotlib inline

"""### Adding Dataset"""

wp = pd.read_csv('Drinking_water.csv')

wp

wp_nrow = wp.shape[0]

def lost_record():

    wp_nrow_now = wp.shape[0]
    lost = wp_nrow - wp_nrow_now
    lost = (lost/wp_nrow)*100
    lost = round(lost, 2)

    return print('We lost ' + str(lost) + ' % data')

"""## Performing EDA In Dataset

### Performing Data Preprocessing

#### Applying Descriptive Statistics
"""

wp.describe()

"""#### Performing Typecasting"""

wp.info()

"""#### Performing Missing Value Imputation"""

def missing_percentage(wp):

    m = wp.isna().sum()
    total = int(wp.shape[0])

    for i in range(len(wp.columns)):
        percentage =round((m[i]/total)*100)

        print(str(wp.columns[i]) + ' has ' + str(percentage) + ' % missing value(' + str(m[i]) + ')')

"""##### Missing Value Percentage"""

missing_percentage(wp)

"""##### Filling Missing Value"""

wp.fillna(value = { 'ph' : wp['ph'].median(),
                    'Sulfate' : wp['Sulfate'].median(),
                    'Trihalomethanes' : wp['Trihalomethanes'].median()},
                    inplace = True)

"""##### Missing Value Percentage(After)"""

missing_percentage(wp)

"""#### Finding Unique Values In Dataset"""

def uni(df):

    for i in range(len(df.columns)):
        print('\n All Unique Value in ' + str(df.columns[i]))
        print(np.sort(df[df.columns[i]].unique()))
        print('Total no of unique values ' +
              str(len(df[df.columns[i]].unique())))

uni(wp)

"""### Checking for Duplicated Values"""

wp[wp.duplicated()]

"""- There is no duplicate vale in are dataset

### Visualizing The Data

#### Ploting Boxplot
"""

fig, ax = plt.subplots(3, 3, figsize = (15,8))

plt.setp(ax[0,0], title = 'PH')
sns.boxplot(wp['ph'], orient = 'h', ax = ax[0,0], color = '#ffadad')

plt.setp(ax[0,1], title = 'Hardness')
sns.boxplot(wp['Hardness'], orient = 'h', ax = ax[0,1], color = '#ffadad')

plt.setp(ax[0,2], title = 'Solids')
sns.boxplot(wp['Solids'], orient = 'h', ax = ax[0,2], color = '#ffadad')

plt.setp(ax[1,0], title = 'Chloramines')
sns.boxplot(wp['Chloramines'], orient = 'h', ax = ax[1,0], color = '#ffadad')

plt.setp(ax[1,1], title = 'Sulfate')
sns.boxplot(wp['Sulfate'], orient = 'h', ax = ax[1,1], color = '#ffd6a5')

plt.setp(ax[1,2], title = 'Conductivity')
sns.boxplot(wp['Conductivity'], orient = 'h', ax = ax[1,2], color = '#ffd6a5')

plt.setp(ax[2,0], title = 'Organic_carbon')
sns.boxplot(wp['Organic_carbon'], orient = 'h', ax = ax[2,0], color = '#ffadad')

plt.setp(ax[2,1], title = 'Trihalomethanes')
sns.boxplot(wp['Trihalomethanes'], orient = 'h', ax = ax[2,1], color = '#ffd6a5')

plt.setp(ax[2,2], title = 'Turbidity')
sns.boxplot(wp['Turbidity'], orient = 'h', ax = ax[2,2], color = '#fdffb6')

plt.tight_layout()

"""#### Ploting Histplot"""

fig, ax = plt.subplots(3, 3, figsize = (15,8))

plt.setp(ax[0,0], title = 'PH')
sns.distplot(wp['ph'], ax = ax[0,0], color = '#e9ff70')

plt.setp(ax[0,1], title = 'Hardness')
sns.distplot(wp['Hardness'], ax = ax[0,1], color = '#ffd670')

plt.setp(ax[0,2], title = 'Solids')
sns.distplot(wp['Solids'], ax = ax[0,2], color = '#ff9770')

plt.setp(ax[1,0], title = 'Chloramines')
sns.distplot(wp['Chloramines'], ax = ax[1,0], color = '#ffd670')

plt.setp(ax[1,1], title = 'Sulfate')
sns.distplot(wp['Sulfate'], ax = ax[1,1], color = '#ffd670')

plt.setp(ax[1,2], title = 'Conductivity')
sns.distplot(wp['Conductivity'], ax = ax[1,2], color = '#ff9770')

plt.setp(ax[2,0], title = 'Organic_carbon')
sns.distplot(wp['Organic_carbon'], ax = ax[2,0], color = '#ff9770')

plt.setp(ax[2,1], title = 'Trihalomethanes')
sns.distplot(wp['Trihalomethanes'], ax = ax[2,1], color = '#ff9770')

plt.setp(ax[2,2], title = 'Turbidity')
sns.distplot(wp['Turbidity'], ax = ax[2,2], color = '#ff9770')

plt.tight_layout()

"""#### Ploting Pie-Plot"""

fig, ax = plt.subplots(figsize = (12,6))

plt.title('Pure VS Contaminated Water')
plt.pie(wp['Potability'].value_counts(), autopct = '%0.2f%%', labels = ['Contaminated Drink', 'Pure water'], colors=['#ef476f', '#ffd166'])

"""### Correlation Analysis"""

sns.pairplot(wp, hue='Potability')

plt.show()

sns.heatmap(wp.corr(), annot = True)

"""### Outlier Detection"""

def outlier_percentage(df):
    for i in range(len(df.columns)):

        q1 = df[df.columns[i]].quantile(0.25)
        q3 = df[df.columns[i]].quantile(0.75)
        iqr = q3-q1

        upper = q3 + (iqr*1.5)
        lower = q1 - (iqr*1.5)

        percentage = (((len(df[df[df.columns[i]] > upper])) +
                      (len(df[df[df.columns[i]] < lower])))/len(df[df.columns[i]]))*100

        print(str(df.columns[i]) + ' : ' + str(percentage) + ' %')

def replace_outlier_extream(st, col):

    q1 = st[col].quantile(0.25)
    q3 = st[col].quantile(0.75)
    iqr = q3-q1

    upper = q3 + (iqr*1.5)
    lower = q1 - (iqr*1.5)

    st[col].mask(st[col] > upper, upper, inplace=True)
    st[col].mask(st[col] < lower, lower, inplace=True)

def remove_outlier(st, col):
    q1 = st[col].quantile(0.25)
    q3 = st[col].quantile(0.75)

    iqr = q3-q1

    upper = q3 + (iqr*1.5)
    lower = q1 - (iqr*1.5)

    st = st[(st[col] > lower) & (st[col] < upper)]

    return st

"""#### Outlier Percentage (Before)"""

outlier_percentage(wp)

"""#### Using Isolation Forest"""

iso = IsolationForest()
iso.fit(wp)
outliers = iso.predict(wp)
outliers

"""##### Creating Dataframe"""

wp['Outliers'] = outliers
wp

wp.drop(wp[wp['Outliers']==-1].index, inplace=True)

"""#### Outlier Percentage (After Using Isolation Forest)"""

outlier_percentage(wp)

"""#### Replacing Outlier in ph Column"""

sns.boxplot(wp['ph'], orient = 'h', color = '#FF5B00')

wp = remove_outlier(wp, 'ph')

sns.boxplot(wp['ph'], orient = 'h', color = '#D4D925')

"""#### Replacing Outlier in Hardness Column"""

sns.boxplot(wp['Hardness'], orient = 'h', color = '#FF5B00')

replace_outlier_extream(wp, 'Hardness')

sns.boxplot(wp['Hardness'], orient = 'h', color = '#D4D925')

"""#### Replacing Outlier in Solids Column"""

sns.boxplot(wp['Solids'], orient = 'h', color = '#FF5B00')

replace_outlier_extream(wp, 'Solids')

sns.boxplot(wp['Solids'], orient = 'h', color = '#D4D925')

"""#### Removing Outlier in Chloramines Column"""

sns.boxplot(wp['Chloramines'], orient = 'h', color = '#FF5B00')

wp = remove_outlier(wp, 'Chloramines')

sns.boxplot(wp['Chloramines'], orient = 'h', color = '#D4D925')

"""#### Replacing Outlier in Sulfate Column"""

sns.boxplot(wp['Sulfate'], orient = 'h', color = '#FF5B00')

replace_outlier_extream(wp, 'Sulfate')

sns.boxplot(wp['Sulfate'], orient = 'h', color = '#D4D925')

"""#### Removing Outlier in Conductivity Column"""

sns.boxplot(wp['Conductivity'], orient = 'h', color = '#FF5B00')

replace_outlier_extream(wp, 'Conductivity')

sns.boxplot(wp['Conductivity'], orient = 'h', color = '#D4D925')

"""#### Replacing Outlier in Organic Carbon Column"""

sns.boxplot(wp['Organic_carbon'], orient = 'h', color = '#FF5B00')

replace_outlier_extream(wp, 'Organic_carbon')

sns.boxplot(wp['Organic_carbon'], orient = 'h', color = '#D4D925')

"""#### Replacing Outlier in Trihalomethanes Column"""

sns.boxplot(wp['Trihalomethanes'], orient = 'h', color = '#FF5B00')

replace_outlier_extream(wp, 'Trihalomethanes')

sns.boxplot(wp['Trihalomethanes'], orient = 'h', color = '#D4D925')

"""#### Replacing Outlier in Turbidity Column"""

sns.boxplot(wp['Turbidity'], orient = 'h', color = '#FF5B00')

replace_outlier_extream(wp, 'Turbidity')

sns.boxplot(wp['Sulfate'], orient = 'h', color = '#D4D925')

"""#### Outlier Percentage (After)"""

outlier_percentage(wp)

"""#### Reindexing Columns"""

wp.reset_index(inplace=True)
wp.drop(columns=['index', 'Outliers'], inplace=True)

wp

"""## Exporting Dataframe"""

wp.to_csv('EDA.csv', index=False)

wp = pd.read_csv('EDA.csv')
wp.drop(columns='Unnamed: 0', inplace = True)

wp

x = wp[['ph','Solids','Turbidity']]
y = wp['Potability']

x

sc = StandardScaler()
scaled_x  = pd.DataFrame(sc.fit_transform(x), columns=x.columns)

scaled_x

xtrain,xtest,ytrain,ytest = train_test_split(scaled_x,y,train_size=0.75,random_state=1)

xtrain

xtrain.to_csv('xtrain.csv', index=False)
xtest.to_csv('xtest.csv', index=False)
ytrain.to_csv('ytrain.csv', index=False)
ytest.to_csv('ytest.csv', index=False)

"""Model Training"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,StackingClassifier, BaggingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
import pickle

from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV

import warnings

warnings.filterwarnings('ignore')
plt.rcParams['figure.figsize'] = (15,8)
plt.rcParams['figure.dpi'] = 250
sns.set_style(style='darkgrid')
plt.tight_layout()
# %matplotlib inline

"""Adding dataset"""

xtrain = pd.read_csv('xtrain.csv')
ytrain = pd.read_csv('ytrain.csv')

xtrain.head()

xtest = pd.read_csv('xtest.csv')
ytest = pd.read_csv('ytest.csv')

xtest.head()

"""Preparing model for dataset

Picking  Model Lowest Training And Testing Difference
"""

def predict(model):
    print('Creating Model With ' + str(model))
    model.fit(xtrain,ytrain)
    ypred = model.predict(xtest)
    print('Training Accuracy of  Model is : ' + str(model.score(xtrain,ytrain)))
    print('Test Accuracy of  Model is : ' + str(model.score(xtest,ytest)))
    print(classification_report(ytest,ypred))

models = [
    LogisticRegression(random_state=42, solver='liblinear'),
    KNeighborsClassifier(),
    SVC(random_state=42, probability=True),
    BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42),
    RandomForestClassifier(random_state=42),
    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1, random_state=42), random_state=42),
    GradientBoostingClassifier(random_state=42),
    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    VotingClassifier(
        estimators=[
            ('lr', LogisticRegression(random_state=42, solver='liblinear')),
            ('knn', KNeighborsClassifier()),
            ('rf', RandomForestClassifier(random_state=42, n_estimators=50))
        ],
        voting='soft',
        n_jobs=-1
    )
]



for model in models:
    predict(model)

"""Applying SMOTE"""

from imblearn.over_sampling import SMOTE
print("--- Addressing Class Imbalance with SMOTE ---")
print(f"Original training class distribution (Potability):\n{ytrain.value_counts()}")
smote = SMOTE(random_state=42)
xtrain_smote, ytrain_smote = smote.fit_resample(xtrain, ytrain)
print(f"\nSMOTE training class distribution:\n{ytrain_smote.value_counts()}")
print("-" * 50)

def predict(model):
    print('Creating Model With ' + str(model))
    model.fit(xtrain_smote,ytrain_smote)
    ypred = model.predict(xtest)
    print('Training Accuracy of  Model is : ' + str(model.score(xtrain_smote,ytrain_smote)))
    print('Test Accuracy of  Model is : ' + str(model.score(xtest,ytest)))
    print(classification_report(ytest,ypred))

models = [
    LogisticRegression(random_state=42, solver='liblinear'),
    KNeighborsClassifier(),
    SVC(random_state=42, probability=True),
    BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42),
    RandomForestClassifier(random_state=42),
    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1, random_state=42), random_state=42),
    GradientBoostingClassifier(random_state=42),
    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    VotingClassifier(
        estimators=[
            ('lr', LogisticRegression(random_state=42, solver='liblinear')),
            ('knn', KNeighborsClassifier()),
            ('rf', RandomForestClassifier(random_state=42, n_estimators=50))
        ],
        voting='soft',
        n_jobs=-1
    )
]



for model in models:
    predict(model)

"""XGboost hyperparameter tuning"""

import joblib
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from xgboost import XGBClassifier


param_grid_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2]
}


grid_search_xgb = GridSearchCV(
    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    param_grid_xgb,
    scoring='f1_macro',
    cv=5,
    verbose=1,
    n_jobs=-1
)

print("--- Starting GridSearchCV for Final XGBClassifier Tuning (This may take a few minutes) ---")
grid_search_xgb.fit(xtrain_smote, ytrain_smote)
best_xgb_model = grid_search_xgb.best_estimator_
ypred_xgb = best_xgb_model.predict(xtest)
print("\nBest XGBoost Parameters:")
print(grid_search_xgb.best_params_)
print(f"\nBest Cross-Validated Macro F1-Score: {grid_search_xgb.best_score_:.4f}")
print("\nFinal Tuned XGBoost Classification Report on Test Data:")
print(classification_report(ytest, ypred_xgb))
MODEL_FILE = 'water_quality_model_tuned_xgb.pkl'
SCALER_FILE = 'scaler_final.pkl'


print("-" * 50)
print("Code for tuning executed. Please check the output to see if the Test Macro F1-Score has improved and the overfitting has been reduced.")



import joblib
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from xgboost import XGBClassifier
print("--- Starting SECOND XGBoost Tuning Iteration: Aggressive Regularization ---")
param_grid_xgb_v2 = {
    'max_depth': [3, 4, 5],
    'n_estimators': [100, 150],
    'learning_rate': [0.1, 0.2],
    'reg_lambda': [1, 5, 10]
}


grid_search_xgb_v2 = GridSearchCV(
    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    param_grid_xgb_v2,
    scoring='f1_macro',
    cv=5,
    verbose=1,
    n_jobs=-1
)

grid_search_xgb_v2.fit(xtrain_smote, ytrain_smote)
best_xgb_model_v2 = grid_search_xgb_v2.best_estimator_
ypred_xgb_v2 = best_xgb_model_v2.predict(xtest)

print("\nBest XGBoost V2 Parameters:")
print(grid_search_xgb_v2.best_params_)
print(f"\nBest Cross-Validated Macro F1-Score V2: {grid_search_xgb_v2.best_score_:.4f}")
print("\nFinal Tuned XGBoost V2 Classification Report on Test Data:")
print(classification_report(ytest, ypred_xgb_v2))

"""Gradient boost"""

import joblib
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import GradientBoostingClassifier

print("--- Starting FINAL Tuning: GradientBoostingClassifier ---")
param_grid_gbc = {
    'n_estimators': [100, 150],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.1, 0.2],
    'subsample': [0.7, 0.9]
}


grid_search_gbc = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_grid_gbc,
    scoring='f1_macro',
    cv=5,
    verbose=1,
    n_jobs=-1
)
grid_search_gbc.fit(xtrain_smote, ytrain_smote)
best_gbc_model = grid_search_gbc.best_estimator_
ypred_gbc = best_gbc_model.predict(xtest)
print("\nBest GradientBoostingClassifier Parameters:")
print(grid_search_gbc.best_params_)
print(f"\nBest Cross-Validated Macro F1-Score: {grid_search_gbc.best_score_:.4f}")
print("\nFinal Tuned GradientBoostingClassifier Report on Test Data:")
print(classification_report(ytest, ypred_gbc))

"""Voting Ensemble"""

import joblib
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import classification_report, accuracy_score
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier

# --- Define the Best Base Estimators ---
# Use the best parameters found during your tuning steps.
best_xgb_tuned = XGBClassifier(
    learning_rate=0.2, max_depth=5, n_estimators=100, reg_lambda=1,
    random_state=42, use_label_encoder=False, eval_metric='logloss'
)

best_gbc_tuned = GradientBoostingClassifier(
    learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.9, random_state=42
)

# SVC often performs well when its errors are combined with ensemble methods
svc_tuned = SVC(probability=True, class_weight='balanced', C=10, gamma='scale', kernel='rbf', random_state=42)

# 1. Create the Voting Classifier
final_voting_model = VotingClassifier(
    estimators=[
        ('xgb', best_xgb_tuned),
        ('gbc', best_gbc_tuned),
        ('svc', svc_tuned)
    ],
    voting='soft', # Soft voting weights the prediction by probability
    n_jobs=-1
)

# 2. Fit the Model on SMOTE-Balanced Data
print("--- Training Final Voting Ensemble on SMOTE-Balanced Data ---")
final_voting_model.fit(xtrain_smote, ytrain_smote)

# 3. Evaluate on the Scaled Test Data
ypred_voting = final_voting_model.predict(xtest)

print('\nFinal Voting Ensemble Classification Report (SMOTE Trained):')
print(classification_report(ytest, ypred_voting))

# 4. Save the Final Model and Scaler
MODEL_FILE = 'water_quality_model_final_voting.pkl'
SCALER_FILE = 'scaler_final.pkl'

joblib.dump(final_voting_model, MODEL_FILE)
# Assuming 'scaler' object is available
# joblib.dump(scaler, SCALER_FILE)

print("-" * 50)
print(f"âœ… Final Ensemble Model Saved for Deployment: {MODEL_FILE}")